{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import pygame\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "from pettingzoo import ParallelEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTrainEnv(ParallelEnv):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, visualize=False):\n",
    "\n",
    "        self.states = [] \n",
    "        self.states.append([0.0, 100.0, 0.0, 150.0])  \n",
    "        self.states.append([0.0, 150.0, 0.0, 200.0])\n",
    "        self.states.append([0.0, 200.0, 0.0, -1.0]) # -1 means no train in front\n",
    "        \n",
    "        # Actions: accelerate, decelerate, keep speed\n",
    "        self.action_space = Discrete(3)\n",
    "        # Observation space includes [speed, position, front_train_speed, front_train_position]\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, 0.0]),\n",
    "            high=np.array([1.0, 900.0, 1.0, 900.0])\n",
    "        )\n",
    "\n",
    "        self.target = np.array([900, 150])\n",
    "        self.visualize = visualize\n",
    "        if visualize:\n",
    "            self.init_pygame()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.states = []\n",
    "        self.states.append([0.0, 100.0, 0.0, 150.0])  \n",
    "        self.states.append([0.0, 150.0, 0.0, 200.0])\n",
    "        self.states.append([0.0, 200.0, 0.0, -1.0])\n",
    "        return self.states\n",
    "\n",
    "    def init_pygame(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((1000, 300))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def step(self, actions, dones):\n",
    "        rewards = []\n",
    "        infos = []\n",
    "\n",
    "        for idx, action in enumerate(actions):\n",
    "            reward = 0  # Default reward\n",
    "            if not dones[idx]:  # Process only if the agent is not done\n",
    "                speed, position, front_train_speed, front_train_position = self.states[idx]\n",
    "\n",
    "                # Action: 0 = accelerate, 1 = decelerate, 2 = maintain speed\n",
    "                if action == 0:  # Accelerate\n",
    "                    speed = min(1.0, speed + 0.005)\n",
    "                elif action == 1:  # Decelerate\n",
    "                    speed = max(0.0, speed - 0.005)\n",
    "\n",
    "                # Update speed and position\n",
    "                self.states[idx][0] = speed\n",
    "                self.states[idx][1] += speed\n",
    "                \n",
    "                # Update previous trains' knowledge on the train in front \n",
    "                if idx > 0:  # Only update if there is a previous train\n",
    "                    self.states[idx - 1][2] = speed\n",
    "                    self.states[idx - 1][3] += speed\n",
    "\n",
    "                # Reward logic\n",
    "                reward = -0.01  # Small penalty for time\n",
    "                if speed > 0.9:\n",
    "                    reward += 0.1  # Reward for high speed\n",
    "\n",
    "                if (front_train_position - position) < 30:\n",
    "                    reward -= 100  # Collision penalty\n",
    "\n",
    "                if (900 - position) < 5:  # Reached destination\n",
    "                    reward += 100\n",
    "                    self.states[idx][2] = -1  # No front train speed\n",
    "                    self.states[idx][3] = -1 \n",
    "                    self.states[idx][0] = 0.0  # Stop at destination\n",
    "                    dones[idx] = True  # Mark this agent as done\n",
    "\n",
    "            rewards.append(reward if not dones[idx] else 0)\n",
    "            infos.append({})  \n",
    "\n",
    "        return self.states, rewards, dones, infos\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Clear the screen\n",
    "        self.screen.fill((0, 0, 0))\n",
    "\n",
    "        # Draw the target\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), (int(self.target[0]), int(self.target[1])), 10)\n",
    "\n",
    "        # Draw the trains\n",
    "        for train in self.states:\n",
    "            train_position = (int(train[1]), 150)  # Assuming vertical position is fixed at 150\n",
    "            pygame.draw.circle(self.screen, (0, 255, 0), train_position, 10)\n",
    "    \n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Cap the frame rate\n",
    "        self.clock.tick(60)\n",
    "\n",
    "    def close(self):\n",
    "        if self.is_pygame_initialized:\n",
    "            print(\"Closing Pygame...\")\n",
    "            pygame.quit()\n",
    "            self.is_pygame_initialized = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model DeepQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        return actions\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0 # memory counter\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(self.lr, input_dims=input_dims, fc1_dims=256, fc2_dims=256, n_actions=n_actions)\n",
    "\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    \n",
    "    # explore or exploit\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # learn as soon as batch size full of memory\n",
    "        if self.mem_cntr < self.batch_size: # if memory counter smaller than batch size\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "\n",
    "        action_batch = self.action_memory[batch]\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "\n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "            else self.eps_min    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_30284\\2670430157.py:48: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Score: -384259.71 | Avg Score: -384259.71 | Epsilon: 0.01\n",
      "Episode 1 | Score: -236267.84 | Avg Score: -310263.78 | Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = MultiTrainEnv(True)\n",
    "\n",
    "# Initialize the agent (same model for all agents)\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=3, eps_end=0.01, input_dims=[4], lr=0.003)\n",
    "\n",
    "# Variables to track performance\n",
    "scores, eps_history = [], []\n",
    "n_games = 2  # Number of training episodes\n",
    "\n",
    "# Loop over episodes\n",
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = [False] * len(env.states)  # Done flag for each agent\n",
    "    observations = env.reset()  # Reset environment and get initial states for all agents (dim: num_agents x 4)\n",
    "\n",
    "    # Game loop for each episode\n",
    "    while not all(done):  # Continue until all agents are done\n",
    "        actions = []  # Store actions for all agents\n",
    "        for agent_id in range(len(env.states)):\n",
    "            if not done[agent_id]:\n",
    "                # Each agent chooses its action based on its observation (state)\n",
    "                action = agent.choose_action(observations[agent_id])\n",
    "                actions.append(action)\n",
    "            else:\n",
    "                actions.append(None)\n",
    "\n",
    "        # Perform actions for all agents in the environment\n",
    "        new_observations, rewards, done, info = env.step(actions, done)\n",
    "        \n",
    "        # Update score for each agent (sum the rewards)\n",
    "        for agent_id in range(len(env.states)):\n",
    "            if not done[agent_id]:\n",
    "                agent.store_transition(observations[agent_id], actions[agent_id], rewards[agent_id],\n",
    "                                    new_observations[agent_id], done[agent_id])\n",
    "                agent.learn()\n",
    "                score += rewards[agent_id]\n",
    "\n",
    "        # Update the observations for the next step\n",
    "        observations = new_observations\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    # Store the score for this episode\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "\n",
    "    # Calculate and print average score over the last 100 games\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(f'Episode {i} | Score: {score:.2f} | Avg Score: {avg_score:.2f} | Epsilon: {agent.epsilon:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
